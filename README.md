### Case iFood
# NYC Yellow Taxi Data - PySpark ETL
**Este projeto realiza a extra√ß√£o, normaliza√ß√£o e transforma√ß√£o de dados de corridas de t√°xi de Nova York (Yellow Taxi), utilizando PySpark e arquivos armazenados em um bucket Amazon S3.**


![Fluxo da ETL](/Workspace/Users/leonardocapriglione@gmail.com/ifood-case/Case-iFood/fluxo.png)


### ‚öôÔ∏è Pipeline de Processamento - SRC
### 
**1. read_files_by_dates_s3_uc_select_columns:**

L√™ os arquivos do bucket S3 com base em uma lista de datas (YYYYMM), e orquestra o pipeline completo:
- L√™ m√∫ltiplos arquivos Parquet no formato `yellow_tripdata_YYYY-MM.parquet`.
- Chama a fun√ß√£o `normalize_dataframe_columns` para padronizar os nomes das colunas.
- Adiciona a coluna `source_file` com o m√™s de origem de cada arquivo.
- Une todos os DataFrames.
- Chama transform_data para enriquecer e limpar os dados.

**2. normalize_dataframe_columns**

Padroniza os nomes das colunas para evitar erros posteriores e garantir consist√™ncia:

- Remove acentos e diacr√≠ticos.
- Remove caracteres especiais (mantendo apenas letras, n√∫meros e underscore).
- Converte tudo para letras min√∫sculas.

**3. transform_data:**

Enriquece e limpa os dados brutos do Yellow Taxi, adicionando colunas mais descritivas e removendo colunas t√©cnicas:

**Traduz c√≥digos das colunas:**

- VendorID ‚Üí vendor_name
- RatecodeID ‚Üí rate_code_name
- store_and_fwd_flag ‚Üí store_and_fwd_desc
- payment_type ‚Üí payment_type_desc

Extrai componentes de data e hora das colunas:

`pickup_date, pickup_time, dropoff_date, dropoff_time`

Remove colunas originais: `tpep_pickup_datetime, tpep_dropoff_datetime`

**4. filter_by_pickup_datetime_range:**

Limpa a base com dados fora do range necess√°rio para a an√°lise. 2023-01-01 √† 2023-05-31.


**Par√¢metros:**

- bucket_name (str): Nome do bucket no S3.
- dates (list[str]): Lista de datas no formato YYYYMM.
- file_format (str): Formato dos arquivos (parquet por padr√£o).

**Retorno:**

Um √∫nico DataFrame contendo todos os dados normalizados e transformados.

**Exemplo de uso:**

Declarar vari√°veis globais:

imports necess√°rios:

` import os`
` import re`
` from pyspark.sql`
`import SparkSession, DataFrame`
` from pyspark.sql.functions import lit, input_file_name, when, col, to_timestamp, date_format`
`from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType`
`from functools import reduce`

`datas = ["202301", "202302", "202303","202304","202305"]` -- Datas necess√°rias para an√°lise.

`bucket = "landing-layer-ifood"` -- bucket que ir√° ler os arquivos.

Execu√ß√£o fun√ß√£o `main()`

üõ†Ô∏è Requisitos

Acesso √† AWS S3 com permiss√µes de leitura

Configura√ß√£o do Spark para uso com s3a:// (ex: via Databricks, EMR ou local com Hadoop configurado)

üìå Observa√ß√µes

A fun√ß√£o normalize_dataframe_columns √© essencial para tratar inconsist√™ncias entre arquivos mensais.

A coluna source_file permite rastrear o m√™s de origem de cada linha do DataFrame final.



### ‚öôÔ∏è Pipeline de An√°lise - Analysis
### 

**Este projeto realiza an√°lises de dados de corridas de t√°xis da frota Yellow Taxi, utilizando PySpark para processar os dados e executar consultas SQL.**

### Objetivos

- Calcular a m√©dia do valor total (total_amount) das corridas por m√™s, filtrando corridas encerradas (com tpep_dropoff_datetime preenchido).

- Calcular a m√©dia do n√∫mero de passageiros (passenger_count) por hora do dia para o m√™s de maio.

**Descri√ß√£o do C√≥digo**

**1. Prepara√ß√£o dos Dados**

Os dados brutos est√£o carregados em um DataFrame PySpark chamado df no notebook `src`

O DataFrame √© registrado como uma view tempor√°ria chamada "yellow_taxi_table" para permitir consultas SQL.

**2. Consulta M√©dia Mensal do Valor Total**

Consulta SQL para calcular a m√©dia do valor total `(total_amount)` agrupada por m√™s (ano e m√™s no formato yyyy-MM), considerando apenas corridas que possuem `tpep_dropoff_datetime` preenchido para retirar corridas que n√£o foram finalizadas.

**3. Consulta M√©dia de Passageiros por Hora (M√™s de Maio)**

Consulta SQL para calcular a m√©dia do n√∫mero de passageiros (passenger_count) por hora do dia, filtrando apenas corridas que ocorreram no m√™s de **maio de 2023**.

üõ†Ô∏è **Como Executar**

- Carregue seus dados em um DataFrame PySpark chamado df.

- Execute o script completo para registrar a view e realizar as consultas.

- As mensagens ser√£o impressas no console com os resultados.

**Requisitos**

- Apache Spark com PySpark configurado
- Dados de corridas Yellow Taxi com as colunas mencionadas (`pickup_date`, `pickup_time`, `total_amount`, `passenger_count`, `tpep_dropoff_datetime`)