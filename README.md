### Case iFood
# NYC Yellow Taxi Data - PySpark ETL
**Este projeto realiza a extra√ß√£o, normaliza√ß√£o e transforma√ß√£o de dados de corridas de t√°xi de Nova York (Yellow Taxi), utilizando PySpark e arquivos armazenados em um bucket Amazon S3.**

### ‚öôÔ∏è Pipeline de Processamento - SRC
### 
**1. read_files_by_dates_s3_uc_select_columns:**

L√™ os arquivos do bucket S3 com base em uma lista de datas (YYYYMM), e orquestra o pipeline completo:
- L√™ m√∫ltiplos arquivos Parquet no formato `yellow_tripdata_YYYY-MM.parquet`.
- Chama a fun√ß√£o `normalize_dataframe_columns` para padronizar os nomes das colunas.
- Adiciona a coluna `source_file` com o m√™s de origem de cada arquivo.
- Une todos os DataFrames.
- Chama transform_data para enriquecer e limpar os dados.

**2. normalize_dataframe_columns**

Padroniza os nomes das colunas para evitar erros posteriores e garantir consist√™ncia:

- Remove acentos e diacr√≠ticos.
- Remove caracteres especiais (mantendo apenas letras, n√∫meros e underscore).
- Converte tudo para letras min√∫sculas.

**3. transform_data:**

Enriquece e limpa os dados brutos do Yellow Taxi, adicionando colunas mais descritivas e removendo colunas t√©cnicas:

**Traduz c√≥digos das colunas:**

- VendorID ‚Üí vendor_name
- RatecodeID ‚Üí rate_code_name
- store_and_fwd_flag ‚Üí store_and_fwd_desc
- payment_type ‚Üí payment_type_desc

Extrai componentes de data e hora das colunas:

`pickup_date, pickup_time, dropoff_date, dropoff_time`

Remove colunas originais: `tpep_pickup_datetime, tpep_dropoff_datetime`

**4. filter_by_pickup_datetime_range:**

Limpa a base com dados fora do range necess√°rio para a an√°lise. 2023-01-01 √† 2023-05-31.


**Par√¢metros:**

- bucket_name (str): Nome do bucket no S3.
- dates (list[str]): Lista de datas no formato YYYYMM.
- file_format (str): Formato dos arquivos (parquet por padr√£o).

**Retorno:**

Um √∫nico DataFrame contendo todos os dados normalizados e transformados.

**Exemplo de uso:**

Declarar vari√°veis globais:

imports necess√°rios:

` import os`
` import re`
` from pyspark.sql`
`import SparkSession, DataFrame`
` from pyspark.sql.functions import lit, input_file_name, when, col, to_timestamp, date_format`
`from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType`
`from functools import reduce`

`datas = ["202301", "202302", "202303","202304","202305"]` -- Datas necess√°rias para an√°lise.

`bucket = "landing-layer-ifood"` -- bucket que ir√° ler os arquivos.

Execu√ß√£o fun√ß√£o `main()`

üõ†Ô∏è Requisitos

Acesso √† AWS S3 com permiss√µes de leitura

Configura√ß√£o do Spark para uso com s3a:// (ex: via Databricks, EMR ou local com Hadoop configurado)

üìå Observa√ß√µes

A fun√ß√£o normalize_dataframe_columns √© essencial para tratar inconsist√™ncias entre arquivos mensais.

A coluna source_file permite rastrear o m√™s de origem de cada linha do DataFrame final.



### ‚öôÔ∏è Pipeline de Analise - Analysis
### 

