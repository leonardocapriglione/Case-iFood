{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f030985d-4ef8-4d82-b168-df288f54d977",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import lit, input_file_name, when, col, to_timestamp, date_format\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763cb0c7-9021-4b7c-9d21-5523ca5db412",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Global  variable"
    }
   },
   "outputs": [],
   "source": [
    "datas = [\"202301\", \"202302\", \"202303\",\"202304\",\"202305\"]\n",
    "bucket = \"landing-layer-ifood\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297f0565-f4f9-44b4-9f42-7b509fbaf320",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "functions"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_dataframe_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Normaliza os nomes das colunas de um DataFrame PySpark.\n",
    "\n",
    "    Operações realizadas:\n",
    "    1. Remove acentos e diacríticos.\n",
    "    2. Remove caracteres especiais, mantendo apenas letras, números e underscore.\n",
    "    3. Converte para minúsculas.\n",
    "\n",
    "    Parâmetros:\n",
    "        df (pyspark.sql.DataFrame): DataFrame com nomes de colunas a normalizar.\n",
    "\n",
    "    Retorna:\n",
    "        pyspark.sql.DataFrame: Novo DataFrame com nomes de colunas normalizados.\n",
    "    \"\"\"\n",
    "    def normalize(col_name):\n",
    "        col_name = unicodedata.normalize('NFKD', col_name).encode('ASCII', 'ignore').decode('utf-8')\n",
    "        col_name = re.sub(r'[^a-zA-Z0-9_]', '', col_name)\n",
    "        return col_name.lower()\n",
    "\n",
    "    new_column_names = [normalize(col) for col in df.columns]\n",
    "\n",
    "    for old_name, new_name in zip(df.columns, new_column_names):\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms coded columns in the NYC Taxi dataset into human-readable descriptive fields\n",
    "    and extracts cleaned pickup/dropoff date and time components, removing the original timestamp columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input Spark DataFrame containing the raw taxi trip data.\n",
    "                        Expected columns include: VendorID, RatecodeID, store_and_fwd_flag, payment_type,\n",
    "                        tpep_pickup_datetime, tpep_dropoff_datetime.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new Spark DataFrame with additional descriptive and timestamp-derived columns:\n",
    "                   - vendor_name\n",
    "                   - rate_code_name\n",
    "                   - store_and_fwd_desc\n",
    "                   - payment_type_desc\n",
    "                   - pickup_date\n",
    "                   - pickup_time\n",
    "                   - dropoff_date\n",
    "                   - dropoff_time\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"vendor_name\",\n",
    "            when(col(\"VendorID\") == 1, \"Creative Mobile Technologies, LLC\")\n",
    "            .when(col(\"VendorID\") == 2, \"Curb Mobility, LLC\")\n",
    "            .when(col(\"VendorID\") == 6, \"Myle Technologies Inc\")\n",
    "            .when(col(\"VendorID\") == 7, \"Helix\")\n",
    "            .otherwise(\"Unknown\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"rate_code_name\",\n",
    "            when(col(\"RatecodeID\") == 1, \"Standard rate\")\n",
    "            .when(col(\"RatecodeID\") == 2, \"JFK\")\n",
    "            .when(col(\"RatecodeID\") == 3, \"Newark\")\n",
    "            .when(col(\"RatecodeID\") == 4, \"Nassau or Westchester\")\n",
    "            .when(col(\"RatecodeID\") == 5, \"Negotiated fare\")\n",
    "            .when(col(\"RatecodeID\") == 6, \"Group ride\")\n",
    "            .when(col(\"RatecodeID\") == 99, \"Null/unknown\")\n",
    "            .otherwise(\"Unknown\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"store_and_fwd_desc\",\n",
    "            when(col(\"store_and_fwd_flag\") == \"Y\", \"Store and forward trip\")\n",
    "            .when(col(\"store_and_fwd_flag\") == \"N\", \"Not a store and forward trip\")\n",
    "            .otherwise(\"Unknown\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"payment_type_desc\",\n",
    "            when(col(\"payment_type\") == 0, \"Flex Fare trip\")\n",
    "            .when(col(\"payment_type\") == 1, \"Credit card\")\n",
    "            .when(col(\"payment_type\") == 2, \"Cash\")\n",
    "            .when(col(\"payment_type\") == 3, \"No charge\")\n",
    "            .when(col(\"payment_type\") == 4, \"Dispute\")\n",
    "            .when(col(\"payment_type\") == 5, \"Unknown\")\n",
    "            .when(col(\"payment_type\") == 6, \"Voided trip\")\n",
    "            .otherwise(\"Unknown\")\n",
    "        )\n",
    "        .withColumn(\"pickup_date\", date_format(\"tpep_pickup_datetime\", \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"pickup_time\", date_format(\"tpep_pickup_datetime\", \"HH:mm:ss\"))\n",
    "        .withColumn(\"dropoff_date\", date_format(\"tpep_dropoff_datetime\", \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"dropoff_time\", date_format(\"tpep_dropoff_datetime\", \"HH:mm:ss\"))\n",
    "    )\n",
    "\n",
    "    df = df.drop(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_files_by_dates_s3_uc_select_columns(bucket_name: str, dates: list, file_format: str = \"parquet\", options: dict = None):\n",
    "    \"\"\"\n",
    "    Lê arquivos do bucket S3, normaliza colunas, e une todos os DataFrames.\n",
    "\n",
    "    Parâmetros:\n",
    "    - bucket_name: nome do bucket S3 (ex: 'landing-layer-ifood')\n",
    "    - dates: lista de datas no formato 'YYYYMM' (ex: ['202301', '202302'])\n",
    "    - file_format: formato dos arquivos (default 'parquet')\n",
    "    - options: opções adicionais para spark.read\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame unificado contendo todas as colunas normalizadas + coluna source_file (YYYY-MM)\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    if options is None:\n",
    "        options = {}\n",
    "\n",
    "    dfs = []\n",
    "    for date in dates:\n",
    "        path = f\"s3a://{bucket_name}/yellow_tripdata_{date[:4]}-{date[4:]}.{file_format}\"\n",
    "        try:\n",
    "            df_temp = spark.read.format(file_format).options(**options).load(path)\n",
    "            df_temp = normalize_dataframe_columns(df_temp)\n",
    "            \n",
    "            match = re.search(r\"yellow_tripdata_(\\d{4}-\\d{2})\", path)\n",
    "            prefix_date = match.group(1) if match else \"unknown\"\n",
    "\n",
    "            df_temp = df_temp.withColumn(\"source_file\", lit(prefix_date))\n",
    "            dfs.append(df_temp)\n",
    "            print(f\"✅ Arquivo carregado e normalizado: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao ler o arquivo {path}: {e}\")\n",
    "\n",
    "    if dfs:\n",
    "        df_union = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs)\n",
    "        return transform_data(df_union)\n",
    "    else:\n",
    "        return spark.createDataFrame([], schema=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ade02a-0e7e-4644-8110-0136940119fd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754608206422}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    The main function of the ETL process is to perform the following steps:\n",
    "\n",
    "    1. Extract data from the S3 bucket by date.\n",
    "    2. Normalize column names.\n",
    "    3. Transform the data (additional adjustments and cleaning).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = read_files_by_dates_s3_uc_select_columns(bucket, datas)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante o processo ETL: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cdeee2e-bc83-46d9-a727-ed3b591ccbb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "src",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
