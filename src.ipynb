{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0241d0c5-baad-443e-a6cc-47ab29a67b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://landing-layer-ifood/yellow_tripdata_2023-01.parquet\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacf3269-d6a8-486c-9d24-449abb14f480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://landing-layer-ifood/yellow_tripdata_2023-02.parquet\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46ccd27-f6c4-4cd8-bbdf-f6c5badefa14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://landing-layer-ifood/yellow_tripdata_2023-03.parquet\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee5ab0e-1ca9-4422-92f6-2118c733f80b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://landing-layer-ifood/yellow_tripdata_2023-04.parquet\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9179bde9-f697-4764-8825-12f99f822844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://landing-layer-ifood/yellow_tripdata_2023-05.parquet\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f030985d-4ef8-4d82-b168-df288f54d977",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, input_file_name\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763cb0c7-9021-4b7c-9d21-5523ca5db412",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Global  variable"
    }
   },
   "outputs": [],
   "source": [
    "datas = [\"202301\", \"202302\", \"202303\",\"202304\",\"202305\"]\n",
    "bucket = \"landing-layer-ifood\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e555f1-4c36-4536-8864-a8685cb7b08a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Functions"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_files_by_dates_s3_uc_select_columns(bucket_name: str, dates: list, file_format: str = \"parquet\", options: dict = None):\n",
    "    \"\"\"\n",
    "    Lê arquivos parquet do bucket S3 e seleciona colunas específicas,\n",
    "    unificando todos os DataFrames resultantes.\n",
    "\n",
    "    Parâmetros:\n",
    "    - bucket_name: nome do bucket S3 (ex: 'landing-layer-ifood')\n",
    "    - dates: lista de datas no formato 'YYYYMM' (ex: ['202301', '202302'])\n",
    "    - file_format: formato dos arquivos (default 'parquet')\n",
    "    - options: opções adicionais para spark.read\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame unificado contendo só as colunas selecionadas + coluna source_file com data no formato YYYY-MM\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    if options is None:\n",
    "        options = {}\n",
    "\n",
    "    cols_to_select = [\n",
    "        \"VendorID\",\n",
    "        \"passenger_count\",\n",
    "        \"total_amount\",\n",
    "        \"tpep_pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\"\n",
    "    ]\n",
    "\n",
    "    dfs = []\n",
    "    for date in dates:\n",
    "        path = f\"s3a://{bucket_name}/yellow_tripdata_{date[:4]}-{date[4:]}.{file_format}\"\n",
    "        try:\n",
    "            df_temp = spark.read.format(file_format).options(**options).load(path)\n",
    "            df_temp = df_temp.select(*cols_to_select)\n",
    "\n",
    "            # Extrai data no formato YYYY-MM do path\n",
    "            match = re.search(r\"yellow_tripdata_(\\d{4}-\\d{2})\", path)\n",
    "            prefix_date = match.group(1) if match else \"unknown\"\n",
    "\n",
    "            df_temp = df_temp.withColumn(\"source_file\", lit(prefix_date))\n",
    "            dfs.append(df_temp)\n",
    "            print(f\"✅ Arquivo carregado e colunas selecionadas: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao ler o arquivo {path}: {e}\")\n",
    "\n",
    "    if dfs:\n",
    "        df_union = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs)\n",
    "        return df_union\n",
    "    else:\n",
    "        return spark.createDataFrame([], schema=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ade02a-0e7e-4644-8110-0136940119fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = read_files_by_dates_s3_uc_select_columns(bucket, datas)\n",
    "df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "src",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
