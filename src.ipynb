{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f030985d-4ef8-4d82-b168-df288f54d977",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763cb0c7-9021-4b7c-9d21-5523ca5db412",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Global  variable"
    }
   },
   "outputs": [],
   "source": [
    "dates_to_load = [\"202301\", \"202302\", \"202303\",\"202304\",\"202305\"]\n",
    "\n",
    "base_path = \"/Workspace/Users/leonardocapriglione@gmail.com/ifood-case/Case-iFood/arquivos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e555f1-4c36-4536-8864-a8685cb7b08a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Functions"
    }
   },
   "outputs": [],
   "source": [
    "def read_files_by_dates(base_path: str, dates: list, file_format: str = \"parquet\", options: dict = None):\n",
    "    \"\"\"\n",
    "    Reads files with the pattern yellow_tripdata_YYYY-MM.{format} based on a list of dates in the format YYYYMM.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Base directory where the files are located.\n",
    "    - dates (list): List of dates in 'YYYYMM' format, e.g., ['202301', '202302'].\n",
    "    - file_format (str): Format of the files (default: 'parquet').\n",
    "    - options (dict): Additional read options as a dictionary.\n",
    "\n",
    "    Returns:\n",
    "    - A unified DataFrame containing the data from all selected files.\n",
    "    \"\"\"\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    if options is None:\n",
    "        options = {}\n",
    "\n",
    "    files_to_read = []\n",
    "    for date in dates:\n",
    "        year = date[:4]\n",
    "        month = date[4:]\n",
    "        file_name = f\"yellow_tripdata_{year}-{month}.{file_format}\"\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        files_to_read.append(file_path)\n",
    "\n",
    "    df_combined = spark.read.format(file_format).options(**options).load(files_to_read)\n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ade02a-0e7e-4644-8110-0136940119fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = read_files_by_dates(base_path, dates_to_load, file_format=\"parquet\")\n",
    "\n",
    "df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "src",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
